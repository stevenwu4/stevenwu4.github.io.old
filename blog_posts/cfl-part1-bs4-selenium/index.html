<!doctype html>

<html lang="en-us">
    <head>
        <meta charset="utf-8">

        <title>Steven Wu</title>

        <meta name="author" content="Steven Wu">
        <meta name="description" content="Personal website for Steven Wu, with blog posts about data">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
        <link rel="stylesheet" href="../../css/minimal.css">
        <link rel="stylesheet" href="../../css/blog.css">

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-58456323-1', 'auto');
          ga('send', 'pageview');
        </script>
    </head>

    <body class="main">
        <div id="banner"></div>
        <header id="header" role="banner">
            <h1><a href="../../">Steven Wu</a></h1>
            <nav>
                <ul>
                    <li><a href="../../">About</a></li>
                    <li><a href="../../blog_posts/">Blog</a></li>
                    <li><a href="https://github.com/stevenwu4">GitHub</a></li>
                    <li><a href="https://www.linkedin.com/profile/view?id=165873568">LinkedIn</a></li>
                    <li><a href="../Steven_Wu_Resume.pdf">Resume</a></li>
                    <li><a href="mailto:steven.wu.work@gmail.com?subject=Emailing from your website">Contact</a></li>
                </ul>
            </nav>
        </header>

        <h2>Data Scraping CFL Play-By-Play Using BeautifulSoup & Selenium in Python</h2>
        <h4>July 27, 2015</h4>

        <div>
            <p>
                My first foray into sports statistics was a research project in 3rd year for STAT5703, Carleton's Data Mining course. This was our first chance at picking our own dataset to work with aside from the canonical cars and housing ones that come bundled with R. When the choices were out for pickings, my partner and I immediately jumped at <a href="http://archive.advancedfootballanalytics.com/2010/04/play-by-play-data.html">Brian Burke's NFL play-by-play</a>. We pored hours into creating new variables, sorting the data into a different format easier for analysis, and finally creating two models; one for probability of winning a given game and the other for number of wins in a given season. It was really satisfying to try a hand at what had always been a blackbox and somewhat succeeding.
            </p>
            <p>
                Amidst this satisfaction, a few things nagged at me. It was really annoying to make the formulas in Excel and do countless actions of copying/pasting. It would have been nice if the data was in the format my partner and I preferred to work with (each season data was in one Excel file; we ended up splitting each up to an Excel file per team per season). I also follow the NBA way more than the NFL, but unfortunately I couldn't find an NBA dataset online as nicely complete as Brian Burke's NFL one. This feeling of restriction, of being handcuffed and limited by the data I was working with, was a theme throughout the rest of the class. This is *not* a knock on the class, as there was hardly enough time to cover what to do once you have obtained the data - let alone how you go about obtaining it (it is probably the case that many people interested in the field of data mining do not care one bit about how you obtain the data). However, it's my contention that web scraping and the associated data processing skills are a necessary part of the toolbox. It allows you to pursue your own ideas - create your own hypothesis to prove or disprove, gather any information you find relevant to your investigation, and format it in whatever way you believe will maximize your success in finding your findings.
            </p>
            <p>
                The aim of this tutorial is to be the tutorial I wish I had stumbled upon when choosing my topic for my aforementioned research project. There are plenty of web scraping 101 tutorials out there: this will one with an application to sports, with particular attention given to the data processing and analysis tasks that are necessary afterwards.

                The environment going forward with this tutorial will be:
                <ul>
                    <li>Python 2.7.5</li>
                    <li>Mac OS X</li>
                    <li>Google Chrome</li>
                </ul>
                where you can find <a href="http://docs.python-guide.org/en/latest/starting/install/osx/">the definitive guide on proper installation of Python here</a>. (If you have a different version of Python, or are on a different Operating System, the necessary tweaks to follow this tutorial are minor and can be found through some Google searches.)
                <br>
                If this is new territory for you, ignore the section on virtual environments. Once you have <em>pip</em> installed, you can run the following commands to install the required libraries to complete this walkthrough.
                <div class="row">
                    <code class="col-10"><pre>
pip install BeautifulSoup4
pip install selenium
pip install docopt</pre></code>
                </div>
                Before we jump into it, there's two guidelines to be conscious of:
                <ol>
                    <li>The site's terms and conditions;
                    some sites explicitly disallow scraping of their content.</li>
                    <li>Every site also has a robots.txt file, which contains rules about which crawlers are allowed/disallowed as well as a crawl delay. Always respect the crawl delay!</li>
                </ol>
            </p>
            <p>
                <h3>The Goal</h3>
                We want to build a system that can
                <ol>
                    <li>gather play-by-play information from the CFL's website</li>
                    <li>store it in an intelligible manner</li>
                    <li>extract new insights</li>
                </ol>
                Specifically, I had a colleague interested in 3rd down plays that don't result in punts (the CFL's possessions only have 3 downs compared to the standard 4 downs).
                <br>
                The end result of the full source code can be found <a href-"https://github.com/stevenwu4/CFL">here: https://github.com/stevenwu4/CFL</a>
            </p>
            <p>
                <h3>Part 1: Implementation of the Scraper</h3>
                <h4>Investigating the Data</h4>
                We want a function that, given a URL from the CFL website, will scrape the play-by-play. Let's look at the first game of the 2015 season for the Ottawa Redblacks: <a href="http://www.cfl.ca/statistics/statsGame/id/12833">http://www.cfl.ca/statistics/statsGame/id/12833</a>.
                <br>
                <div class="row">
                    <img class="col-10" src="../../img/cfl-part1-bs4-selenium/1_playbyplay.png" alt="Image of play-by-play">
                </div>
                <br>
                All a web scraper does is open up a URL's content - much like a human would with a browser like Google Chrome or Mozilla Firefox - and proceed to extract information from the source content. On the web page, right click and click 'View Source'. Here, we run into our first roadblock: the play-by-play doesn't appear to be in the source! Try CMD + F for the first few plays in the game:
                <ul>
                    <li>"Medlock kicks off"</li>
                    <li>"Burris pass to"</li>
                    <li>"Burris incomplete pass"</li>
                </ul>
                None of these will have any results found.
                <br>
                <div class="row">
                    <img class="col-10" src="../../img/cfl-part1-bs4-selenium/2_0searchresults.png" alt="No results for play-by-play found in source!">
                </div>
                <br>
                That's weird; if we click the <b>Play by Play</b> button, we see the play-by-play data right there. How can it be there on the webpage and yet at the same time not be there? Pay attention to the URL; if it changed upon clicking the button, we could just follow the redirect to the new URL and scrape that URL's content - but it doesn't change.
            </p>
            <p>
                <h4>Writing the Scraper - Dealing with the Play By Play Button</h4>
                To go ahead with one solution to this problem, we're going to be using a package called <em><a href="https://selenium-python.readthedocs.org/getting-started.html">Selenium</a></em>. Selenium allows us to use Python to programatically open up a browser and interact with the different elements of the page. Specifically, it will allow us to click the elusive button that contains the play-by-play HTML we desire.
                <br>
                <em><a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a></em> is a library that will take the resulting HTML after the <b>Play by Play</b> button click and allow us easier, more organized access to the information we actually want.
                <div class="row">
                    <code class="col-10"><pre>
import time
import csv
from bs4 import BeautifulSoup
from selenium import webdriver
PLAYBYPLAY_BTN_XPATH = '//div[@id='playbyplay-button']/a'</pre></code>
                </div>
                The <em>time</em> library is necessary to respect the rules of <a href="http://www.cfl.ca/robots.txt">http://www.cfl.ca/robots.txt</a>. At the time of this writing, all crawlers are allowed but there is a crawl delay of 2. The <em>time</em> library has a function, <em>sleep</em>, which lets us delay our crawls.
                <br>
                The constant <em>PLAYBYPLAY_BTN_XPATH</em> is an <a href="https://developer.mozilla.org/en-US/docs/Web/XPath">XPath, a language that allows for specifically identifying elements in our HTML document</a>. We use it to locate the <b>Play by Play</b> button. This begs the question of how to derive the XPath. There's two options here:
                <ol>
                    <li>Use references (the link above is a good start, but any of the top results for Googling XPath work fine too) to learn the language. Like any language, it will take getting used to but once you are comfortable with it you'll never look back.</li>
                    <li>Use auxiliary tools, like Google Chrome's Dev Tools (which can be activated by right-clicking the page and selecting <em>Inspect Element</em>) which allows you a right-click option of 'Copy XPath' in the Elements tab (see image below), or popular browser extensions such as <a href="https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en">XPath Helper</a> that can directly give you a working XPath to use for any element on the page.</li>
                </ol>
                <div class="row">
                    <img class="col-10" src="../../img/cfl-part1-bs4-selenium/3_devtools_copyxpath.png" alt="Image of screen when using Chrome's Dev Tools to copy an XPath">
                </div>
                <br>
                Now we can start creating the function.
                <div class="row">
                    <code class="col-10"><pre>
def get_game_rows_from_url(url):
    driver = webdriver.Firefox()
    driver.get(url)
    time.sleep(10)
    playbyplay_btn = driver.find_element_by_xpath(PLAYBYPLAY_BTN_XPATH)
    playbyplay_btn.click()
    soup = BeautifulSoup(driver.page_source)</pre></code>
                </div>
                Our function <em>get_game_rows_from_url</em> will receive a <em>url</em> as input. We use the <em>Selenium</em> package's <em>webdriver</em> to open up an instance of Firefox on your computer (note: you need <a href="https://www.mozilla.org/en-US/firefox/new/">Firefox downloaded</a> for this to work!). <em>find_element_by_xpath</em> is a function of the webdriver Firefox instance that returns an element of the HTML based on the XPath you give it (in our case, the XPath to the <b>Play by Play</b> button that we defined as <em>PLAYBYPLAY_BTN_XPATH</em>). This returned element has a function <em>click</em> that clicks our button for us. We load the resulting HTML into <em>BeautifulSoup</em> to focus on the scraping.
            </p>
            <p>
                <h4>Writing the Scraper: Extracting the Play By Play</h4>
                <div class="row">
                    <code class="col-10"><pre>
    # Get away/home teams
    away_div = soup.find('div', id='awayteam')
    away_team = away_div.find('h3', class_='cityname').text
    home_div = soup.find('div', id='hometeam')
    home_team = home_div.find('h3', class_='cityname').text

    # Get game rows
    pbp_div = soup.find('div', id='stat-game-pbp')
    pbp_inner_div = pbp_div.find('div', id='pbp-stats')
    pbp_table = pbp_inner_div.find('table', id='pbp-table')
    rows = pbp_table.find_all('tr')

    all_times = []
    all_downs = []
    all_types = []
    all_yards = []
    all_details = []
    all_aways = []
    all_homes = []

    for row in rows:
        # The rows we care about don't have the th tag
        if row.th:
            continue
        cells = row.find_all('td')
        all_times.append(cells[2].text.strip().encode())
        all_downs.append(cells[3].text.strip().encode())
        all_types.append(cells[4].text.strip().encode())
        all_yards.append(cells[5].text.strip().encode())
        all_details.append(cells[6].text.strip().encode())
        all_aways.append(cells[7].text.strip().encode())
        all_homes.append(cells[8].text.strip().encode())

    header_row = [
        'Time', 'Down', 'Type', 'Yards',
        'Details', away_team, home_team
    ]
    list_of_game_rows = [header_row]

    for t, down, types, yards, details, away, home in zip(
        all_times, all_downs, all_types, all_yards,
        all_details, all_aways, all_homes
    ):
        new_row = []
        new_row.append(t)
        new_row.append(down)
        new_row.append(types)
        new_row.append(yards)
        new_row.append(details)
        new_row.append(away)
        new_row.append(home)
        list_of_game_rows.append(new_row)

    driver.close()

    return list_of_game_rows</pre></code>
                </div>
                BeautifulSoup4 has excellent documentation, but it can be daunting if you are new to the concepts or new to Python itself. Two rules of thumb that made it easiest for me when getting the hang of things: 1) The <em><a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/#find">find</a></em> and <em><a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all">find_all</a></em> functions are your best friend, and 2) starting from the soup you obtain by feeding in the original HTML, any resulting object from using <em>find</em> can also use <em>find</em> on itself. For an example of what I'm talking about, let's look at the 3 lines of code that lead to us getting the table containing the play-by-play data, <em>pbp_table</em>. Below is a screenshot that shows how we arrive at getting the rows.
                <div class="row">
                    <img class="col-10" src="../../img/cfl-part1-bs4-selenium/4_pbphtmlstructure.png" alt="Image of HTML of play-by-play">
                </div>
                <br>
                Note: you can consolidate the lines of code that use the above strategy of recursively using <em>find</em> on elements by using XPaths. I strictly use XPaths now after becoming more comfortable with this type of work. Examplem XPaths that would return an equivalent list to <em>rows</em>are '//div[@id="stat-game-pbp"]/div[@id="stat-game-cat"]/table[@id="pbp-table"]//tr' or the simpler '//table[@id="pbp-table"]//tr'
                <br>
                We grab the away and home teams that are playing the specific game. We identify the table that will contain the rows of our play-by-play. From there, we grab all of the data we care about and store the play-by-play as a list of lists, where each inner list is a row of the play-by-play. Finally, we close the Firefox instance that we opened.
            </p>

            <p>
                <h4>The Result</h4>
                <div class="row">
                    <img class="col-10" src="../../img/cfl-part1-bs4-selenium/5_pbpcsv.png" alt="Image of final play-by-play">
                </div>
                To view the final version of the file, visit <a href="https://github.com/stevenwu4/CFL/blob/master/Services/pbp_scraper_for_game.py">https://github.com/stevenwu4/CFL/blob/master/Services/pbp_scraper_for_game.py</a>.
            <br>
            A couple of notes on differences you'll see:
            <ul>
                <li>There's an added input parameter <em>save_to_dest</em> to the function <em>get_game_rows_from_url</em>, which optionally saves the HTML content locally to whatever path value the parameter specifies.
                <li>There's a function imported, <em>write_to_csv</em>, which will take the list of lists and convert it to a .csv for storage.</li>
                <li>Using the extremely handy library <em>docopt</em>, the script can be used from the command line. For example, in the /CFL directory, you would type:
                <div class="row">
                    <code class="col-10">
                    PYTHONPATH=. python pbp_scraper_for_game.py --url=http://www.cfl.ca/statistics/statsGame/id/12833 --final_name=blah.csv
                    </code>
                </div>
            </li>
            </ul>
                Now that we're done implementing our scraper, we need to use it to grab the data. Now, we *could* just manually take every URL for a season, and call this script from the command line to have the play-by-play we want. However, it's good practice to automate as much as possible in our workflow. In the next part of this series, I will go over how to flesh out a basic system that will take care of a workflow that will use this scraper to collect the data and store it locally onto your machine</a>.

                <a href="../cfl-part2-workflow-storage">Click here for Part 2 in this series</a>
            </p>
        </div>
    <footer>
            © Steven Wu<a class="float-right" href="http://minimalcss.com">Design by Minimal</a>
    </footer>
    </body>
</html>
